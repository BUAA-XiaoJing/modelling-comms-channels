\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{amsfonts}
\numberwithin{equation}{section}
\usepackage[numbered, framed]{matlab-prettifier}

\lstset{
	style              = Matlab-editor,
	escapechar         = ",
	mlshowsectionrules = true,
	tabsize            = 2,
}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\setlength{\droptitle}{-8em}

\title{Engineering Computation Project \\ \large Modelling Wireless Communication Channels}

\author{\textbf{Wojciech Dziwulski, Mansfield College}}

\date{}

\begin{document}

\maketitle

\section{Introduction}

This report explores the properties of various orthogonal function sets and their applicability to generating fits to real-life data.

\section{Orthogonality}

Orthogonality is a crucial mathematical concept and extends from the well-known geometrical application (vectors) to the analysis of functions.
\\ \\
We can define a set of orthogonal functions $g_0, g_1, \ldots, g_n$ whose linear combination can be used to fit an arbitrary function f:
\begin{equation}f(x) = a_0 g_0(x)+a_1 g_1(x)+a_2 g_2(x)+\ldots+a_n g_n \end{equation}
\noindent We then define an inner product operation, for which, conveniently:
\begin{equation} \langle\,g_n,g_m\rangle = 0 \quad \textrm{if} \quad n \ne m \end{equation}
So that if we compute the inner product of both LHS and RHS with some arbitrary $g_k$, like:
\begin{equation} \langle\,f,g_k\rangle = a_0\langle\,g_0,g_k\rangle + a_1\langle\,g_1,g_k\rangle + \ldots + \boldsymbol{a_k\langle\,g_k,g_k\rangle} + \ldots + a_n\langle\,g_n,g_k\rangle \end{equation}
which lets us calculate the $a_k$ coefficient as:
\begin{equation} a_k = \frac {\langle\,f,g_k\rangle} {\langle\,g_k,g_k\rangle} \end{equation}
This result is crucial, as it demonstrates that the coefficients are independent of the size of the orthogonal basis used for the investigation i.e. if more functions are decided to be added, old coefficients do not have to be recomputed.
\\ \\
Investigation of orthogonal function fitting can be started with analysing the Fourier series. The set of sines and cosines making up the basis turn out to be orthogonal under integral operation over one period.
\\
Even though useful, Fourier series is quite a basic method for function fitting and will not be analysed in detail in this report. We will instead delve into the investigation of two particularly relevant orthogonal basis functions - Gram-Schmidt and Laguerre.

\section{Gram-Schmidt functions}

\subsection{Process}

The Gram-Schmidt process stems from a very simple logic for generating orthogonal functions - choosing a linearly independent basis, and adjusting the consecutive functions appropriately, making sure that every subsequent function is orthogonal to the previous one.
\\
Hence having the linearly independent basis $v_0, v_1, v_2, \ldots, v_n$ we can compute the orthogonal functions $g_0, g_1, g_2, \ldots, g_n$ using:
\begin{align}
g_0(x) &= v_0(x) \\
g_1(x) &= v_1(x) - e_{10} g_0 (x) \\
g_2(x) &= v_2(x) - e_{20} g_0 - e_{21} g_1 (x) \\
g_3(x) &= v_3(x) - e_{30} g_0 - e_{31} g_1 (x) - e_{32} g_2 (x)
\end{align}

\noindent As noted before, using the orthogonality property we can now compute the e coefficients:
\begin{equation} g_n(x) = v_n(x) - e_{n0} g_0 - \ldots - \boldsymbol{e_{nm} g_m (x)} - \ldots - e_{n(n-1)} g_{n-1} (x) \end{equation}
Taking the inner products:
\begin{align}
\langle\,g_n,g_m\rangle &= \langle\,v_n,g_m\rangle - e_{n0} \langle\,g_0,g_m\rangle &- \ldots &- \boldsymbol{e_{nm} \langle\,g_m,g_m\rangle} &- \ldots &- e_{n(n-1)} \langle\,g_{n-1},g_m\rangle \\
0 &= \langle\,v_n,g_m\rangle - 0 &- \ldots &- \boldsymbol{e_{nm} \langle\,g_m,g_m\rangle} &- \ldots &- 0
\end{align}
\begin{equation} \Rightarrow e_{nm} = \frac{\langle\,v_n,g_m\rangle}{\langle\,g_m,g_m\rangle} \end{equation}
We can then divide each function by its norm (square root of its inner product with itself) in order to achieve an orthonormal set.

\subsection{Monomial basis}

The independent basis chosen is a set of monomials $v_0 = 1, v_1 = x, v_2 = x^2, \ldots, v_n=x^n$ which will be used to produce an orthogonal basis under operation $\int_{0}^{\infty} g_n g_m e^{-x} dx$.\\
We compute the polynomial coefficients by iteratively multiplying the previous orders' coefficient vectors by the e-scaling factor:

\lstinputlisting[caption={gs\_polynomials.m},label={lst:gspolynomials},firstline=32, lastline=46]{../Matlab/GramSchmidt/gs_polynomials.m}

\noindent The leading coefficient of the appropriate orthogonal polynomial is retrieved from the matrix of coefficients of the independent basis ("ones" in listing \ref{lst:gspolynomials}). Interestingly, the Gram Schmidt functions are stored as their polynomial coefficients instead of the y-values calculated over the supplied x-range. \\
The x-range used for the computation of the Gram Schmidt e factors was chosen so that the marginal improvement over the evaluation accuracy did not exceed 1 \% of the function value, and turned out to be around 70. \\ \\
All the functions were tested in the gs\_polynomials\_testbench.m file.

\section{Laguerre functions}

Laguerre polynomials are another example of an orthogonal set, under the inner product defined as:
\begin{equation}
\langle\,L_n^{(\alpha)}, L_m^{(\alpha)}\rangle=\int_{0}^{\infty} x^{\alpha} e^{-x} L_n^{(\alpha)}(x), L_m^{(\alpha)}(x)dx = 
\begin{cases} 
      \frac{\Gamma(n+\alpha+1)}{\Gamma(n+1)} \quad \textrm{for} \quad m = n \\
      0 \quad \quad \quad \quad \quad \textrm{otherwise}
\end{cases}
\end{equation}
We are going to explore two methods of computing their coefficients and subsequently fit them to a real dataset.

\subsection{Rodrigues formula}

Using the Rodrigues formula, the n-th associated Laguerre polynomial is:
\begin{equation}
L_n^{(\alpha)}(x)=\frac{1}{n!}x^{-\alpha}e^x\frac{d^n}{dx^n}(x^{n+\alpha}e^{-x}), \quad\quad n \in \mathbb{N}, \alpha \in \mathbb{R}
\end{equation}
noting that after differentiation the $x^{-\alpha}$ and ${e^x}$ cancel out, all we have to do is calculate the coefficients of the differentiated product using the Leibniz rule:
\begin{equation}
(f \cdot g)^{(n)} = \sum_{k=1}^{n} \binom{n}{k} f^k g^{n-k}
\end{equation}
This is accomplished by first recursively calculating the $x^n$ coefficients and then calculating them by the rest of the appropriate product:

\lstinputlisting[caption={rodrigues\_laguerre.m},label={lst:rodrigueslaguerre},firstline=23, lastline=30]{../Matlab/Laguerre/rodrigues_laguerre.m}

Noting, however, that the Rodrigues formula is computationally inefficient due to the repeated calculation of the derivative coefficients, we turn onto a different method: the recursive method.

\subsection{Recursive method}

This method takes the advantage of the successive polynomial generation using the formula:
\begin{equation}
n L_n^{(\alpha)}(x) = (2n + \alpha + 1 - x) L_{n-1}^{(\alpha)}(x) - (n + \alpha - 1) L_{n-2}^{(\alpha)}(x)
\end{equation}
with:
\begin{align}
L_0^{(\alpha)}(x) &= 1 \\
L_1^{(\alpha)}(x) &= -x+\alpha+1
\end{align}
This is achieved by the fairly simple:
\lstinputlisting[caption={recursive\_laguerre.m},label={lst:recursivelaguerre},firstline=27, lastline=35]{../Matlab/Laguerre/recursive_laguerre.m}

\noindent Note the use of the \textit{circshift} function in order to simulate multiplication by the coefficient vector by the variable $x$.\\
For $\alpha = 0$ the Laguerre plots using the method above looked exactly as expected.

\subsection{Gram-Schmidt Laguerre comparison}

It is easy to note that under $alpha = 0$ the inner product operations for Laguerre and Gram-Schmidt polynomials look identical. This motivated further investigation of both of the orthogonal sets which yielded:

\end{document}